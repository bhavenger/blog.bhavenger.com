+++
date = "2016-06-05T15:18:26+03:00"
description = "Как это работает?"
draft = true
tags = ["hadoop", "101"]
title = "Hadoop 101, часть 2"
topics = ["hadoop"]

+++
## Разбираемся на примере ##
Допустим у нас есть некоторый сервис, который в работе использует данные, которые нам нужны для последующей обработки. Мы не можем читать эти данные напрямую с продакшн базы данных, там высокая нагрузка на запись и чтение, а также, для усложнения ситуации, мы часто чистим нашу бд, оставляя данные только за 3 месяца. Мы также хотим использовать данные из другого нашего сервиса, который и так складывает их в очередь сообщений - kafka. Логичный вывод - нужно на регулярной основе сохранять данные из бд и из очереди в некий сторадж, который легко наращивать, просто добавляя сервера с дисками. В этой картине мира HDFS - то самое хранилище.  
Теперь нам нужна какая-нибудь утилита для того, чтобы выгружать эти данные из бд. И как-то забирать другие данные из второго сервиса. Тут на сцену выходит apache sqoop. Sqoop - это утилита, написанная специально для выгрузки данных из RDBMS в HDFS.  
Ок, а как забирать данные из kafka в hdfs? Apache flume - вот решение! Flume - это такой распределённый сервис, который обрабатывает очередь событий и что-то с ними делает. Мы будем его использовать, чтобы разбирать очереди сообщений в kafka и складывать эти сообщения на HDFS.  
Хорошо, мы складываем данные на HDFS, они там копятся - но нам нужно соединять их в один большой датасет, чтобы считать по нему наши аналитические запросы. Раз мы получаем данные с некоторой регулярность - разумно и соединять их с такой же регулярностью. Нам нужна какая-то утилита, которая возьмёт два датасета и по определённым правилам склеит их в один.

## HDFS ##
Разберём "сердце" hadoop-экосистемы - распределенную файловую систему HDFS.